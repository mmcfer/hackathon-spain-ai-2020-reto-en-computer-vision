{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SpainAI - Reto Super-Resolution 16 parts DeFiAN v2  evaluation.ipynb","provenance":[{"file_id":"1Dswzi0HJvyxJBx8hgHCk8qhQj7qZFDDP","timestamp":1616083596913},{"file_id":"170J7GpSXwQ_o_91NwKSKl00jH79b3Xl9","timestamp":1615992096545},{"file_id":"19tkxpOpHtnBhCGeqG5GwoQaJnW-B_0x2","timestamp":1615563331042},{"file_id":"1fBrZ75J-yvcIFTMuf_Tn0dQB8P2I6sTB","timestamp":1615301028039},{"file_id":"1ZzDg53YtgBHbP6pT0F_iWFATTg_ek1Rs","timestamp":1615151189646},{"file_id":"1WB6ARB3K1hBX5LictLr8Y34FK34051Tp","timestamp":1614987658391},{"file_id":"1gme_omr32nchi-OoudzY-pXPr7-DYLvx","timestamp":1614944212606},{"file_id":"1uNHwkxj4R_vzL8L2mcnhlayNEN2ErANC","timestamp":1614367510521},{"file_id":"1ApsNCz8nuNUr6z8YLNeJAZO6LLq0IzKo","timestamp":1613729679958}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kHc0x4db4WxS"},"source":["Importació de llibreries\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4b47Rio4NPG","executionInfo":{"status":"ok","timestamp":1617395824204,"user_tz":-120,"elapsed":1726,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}},"outputId":"0d995e09-63ce-44e4-9895-102972aee6b9"},"source":["!nvidia-smi -L"],"execution_count":1,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-90519497-ceb0-2bcf-3286-a8918e1af3a6)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gnIjlEgMF5fS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617395850925,"user_tz":-120,"elapsed":28425,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}},"outputId":"82fd20bb-65e1-47b4-96a5-5712479f9fb6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l6DOOH7HwchP","executionInfo":{"status":"ok","timestamp":1617395853657,"user_tz":-120,"elapsed":31152,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torchsummary import summary\n","from torch import Tensor\n","from torch.jit.annotations import List, Tuple\n","\n","\n","import torchvision.transforms.functional as TF\n","\n","\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","from torchvision import transforms\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","import matplotlib.pyplot as plt\n","import random\n","import argparse\n","import os\n","import sys\n","import numpy as np\n","import numbers\n","from collections.abc import Sequence\n","\n","from math import exp\n","import numpy\n","import math\n","import scipy.signal\n","import scipy.ndimage\n","import datetime\n","import time"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ga_OHJL4dEa"},"source":["Definició Classes"]},{"cell_type":"code","metadata":{"id":"BAqau9Mnwn60","executionInfo":{"status":"ok","timestamp":1617395855302,"user_tz":-120,"elapsed":32792,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}}},"source":["# class to extract features for the input image in a different image space like VGG19, AlexNet, DenseNet, etc.\n","class FeatureExtractor(nn.Module):\n","    \n","    def __init__(self, cnn, feature_layer=11):\n","        super(FeatureExtractor, self).__init__()\n","        self.features = nn.Sequential(*list(cnn.features.children())[:(feature_layer+1)])\n","\n","    def forward(self, x):\n","        return self.features(x)\n","\n","def save_img(x, dir):\n","    x = x.cpu().numpy().astype(np.float32)\n","    x_min = np.min(x)\n","    x_max = np.max(x)\n","    x = (x - x_min) / (x_max - x_min)\n","    x = Image.fromarray(x * 255)\n","    if x.mode != 'RGB':\n","        x = x.convert('RGB')\n","    x.save(dir)\n","\n","\n","def calc_mean_std(feat, eps=1e-5):\n","    # eps is a small value added to the variance to avoid divide-by-zero.\n","    size = feat.size()\n","    assert (len(size) == 4)\n","    N, C = size[:2]\n","    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n","    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n","    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n","    return feat_mean, feat_std\n","\n","\n","class DAC(nn.Module):\n","    def __init__(self, n_channels):\n","        super(DAC, self).__init__()\n","\n","        self.mean = nn.Sequential(\n","            nn.Conv2d(n_channels, n_channels // 16, 1, 1, 0, 1, 1, False),\n","            # nn.BatchNorm2d(n_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(n_channels // 16, n_channels, 1, 1, 0, 1, 1, False),\n","            # nn.BatchNorm2d(n_channels),\n","        )\n","        self.std = nn.Sequential(\n","            nn.Conv2d(n_channels, n_channels // 16, 1, 1, 0, 1, 1, False),\n","            # nn.BatchNorm2d(n_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(n_channels // 16, n_channels, 1, 1, 0, 1, 1, False),\n","            # nn.BatchNorm2d(n_channels),\n","        )\n","\n","    def forward(self, observed_feat, referred_feat):\n","        assert (observed_feat.size()[:2] == referred_feat.size()[:2])\n","        size = observed_feat.size()\n","        referred_mean, referred_std = calc_mean_std(referred_feat)\n","        observed_mean, observed_std = calc_mean_std(observed_feat)\n","\n","        normalized_feat = (observed_feat - observed_mean.expand(\n","            size)) / observed_std.expand(size)\n","        referred_mean = self.mean(referred_mean)\n","        referred_std = self.std(referred_std)\n","        output = normalized_feat * referred_std.expand(size) + referred_mean.expand(size)\n","        return output\n","\n","\n","class MSHF(nn.Module):\n","    def __init__(self, n_channels, kernel=3):\n","        super(MSHF, self).__init__()\n","\n","        pad = int((kernel - 1) / 2)\n","\n","        self.grad_xx = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, stride=1, padding=pad,\n","                                 dilation=pad, groups=n_channels, bias=True)\n","        self.grad_yy = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, stride=1, padding=pad,\n","                                 dilation=pad, groups=n_channels, bias=True)\n","        self.grad_xy = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, stride=1, padding=pad,\n","                                 dilation=pad, groups=n_channels, bias=True)\n","\n","        for m in self.modules():\n","            if m == self.grad_xx:\n","                m.weight.data.zero_()\n","                m.weight.data[:, :, 1, 0] = 1\n","                m.weight.data[:, :, 1, 1] = -2\n","                m.weight.data[:, :, 1, -1] = 1\n","            elif m == self.grad_yy:\n","                m.weight.data.zero_()\n","                m.weight.data[:, :, 0, 1] = 1\n","                m.weight.data[:, :, 1, 1] = -2\n","                m.weight.data[:, :, -1, 1] = 1\n","            elif m == self.grad_xy:\n","                m.weight.data.zero_()\n","                m.weight.data[:, :, 0, 0] = 1\n","                m.weight.data[:, :, 0, -1] = -1\n","                m.weight.data[:, :, -1, 0] = -1\n","                m.weight.data[:, :, -1, -1] = 1\n","\n","        # # Freeze the MeanShift layer\n","        # for params in self.parameters():\n","        #     params.requires_grad = False\n","\n","    def forward(self, x):\n","        fxx = self.grad_xx(x)\n","        fyy = self.grad_yy(x)\n","        fxy = self.grad_xy(x)\n","        hessian = ((fxx + fyy) + ((fxx - fyy) ** 2 + 4 * (fxy ** 2)) ** 0.5) / 2\n","        return hessian\n","\n","\n","class rcab_block(nn.Module):\n","    def __init__(self, n_channels, kernel, bias=False, activation=nn.ReLU(inplace=True)):\n","        super(rcab_block, self).__init__()\n","\n","        block = []\n","\n","        block.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel, padding=1, bias=bias))\n","        block.append(activation)\n","        block.append(nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel, padding=1, bias=bias))\n","\n","        self.block = nn.Sequential(*block)\n","\n","        self.calayer = nn.Sequential(\n","            nn.Conv2d(n_channels, n_channels // 16, 1, padding=0, bias=True),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(n_channels // 16, n_channels, 1, padding=0, bias=True),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        residue = self.block(x)\n","        chnlatt = F.adaptive_avg_pool2d(residue, 1)\n","        chnlatt = self.calayer(chnlatt)\n","        output = x + residue * chnlatt\n","\n","        return output\n","\n","\n","class DiEnDec(nn.Module):\n","    def __init__(self, n_channels, act=nn.ReLU(inplace=True)):\n","        super(DiEnDec, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(n_channels, n_channels * 2, kernel_size=3, padding=1, dilation=1, bias=True),\n","            act,\n","            nn.Conv2d(n_channels * 2, n_channels * 4, kernel_size=3, padding=2, dilation=2, bias=True),\n","            act,\n","            nn.Conv2d(n_channels * 4, n_channels * 8, kernel_size=3, padding=4, dilation=4, bias=True),\n","            act,\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(n_channels * 8, n_channels * 4, kernel_size=3, padding=4, dilation=4, bias=True),\n","            act,\n","            nn.ConvTranspose2d(n_channels * 4, n_channels * 2, kernel_size=3, padding=2, dilation=2, bias=True),\n","            act,\n","            nn.ConvTranspose2d(n_channels * 2, n_channels, kernel_size=3, padding=1, dilation=1, bias=True),\n","            act,\n","        )\n","        self.gate = nn.Conv2d(in_channels=n_channels, out_channels=1, kernel_size=1)\n","\n","    def forward(self, x):\n","        output = self.gate(self.decoder(self.encoder(x)))\n","        return output\n","\n","\n","class SingleModule(nn.Module):\n","    def __init__(self, n_channels, n_blocks, act, attention):\n","        super(SingleModule, self).__init__()\n","        res_blocks = [rcab_block(n_channels=n_channels, kernel=3, activation=act) for _ in range(n_blocks)]\n","        self.body_block = nn.Sequential(*res_blocks)\n","        self.attention = attention\n","        if attention:\n","            self.coder = nn.Sequential(DiEnDec(3, act))\n","            self.dac = nn.Sequential(DAC(n_channels))\n","            self.hessian3 = nn.Sequential(MSHF(n_channels, kernel=3))\n","            self.hessian5 = nn.Sequential(MSHF(n_channels, kernel=5))\n","            self.hessian7 = nn.Sequential(MSHF(n_channels, kernel=7))\n","\n","    def forward(self, x):\n","        sz = x.size()\n","        resin = self.body_block(x)\n","\n","        if self.attention:\n","            hessian3 = self.hessian3(resin)\n","            hessian5 = self.hessian5(resin)\n","            hessian7 = self.hessian7(resin)\n","            hessian = torch.cat((torch.mean(hessian3, dim=1, keepdim=True),\n","                                 torch.mean(hessian5, dim=1, keepdim=True),\n","                                 torch.mean(hessian7, dim=1, keepdim=True))\n","                                , 1)\n","            hessian = self.coder(hessian)\n","            attention = torch.sigmoid(self.dac[0](hessian.expand(sz), x))\n","            resout = resin * attention\n","        else:\n","            resout = resin\n","\n","        output = resout + x\n","\n","        return output\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, n_channels, n_blocks, n_modules, act=nn.ReLU(True), attention=True, scale=(2, 3, 4)):\n","        super(Generator, self).__init__()\n","        self.n_modules = n_modules\n","        self.input = nn.Conv2d(in_channels=3, out_channels=n_channels, kernel_size=3, stride=1, padding=1, bias=True)\n","        if n_modules == 1:\n","            self.body = nn.Sequential(SingleModule(n_channels, n_blocks, act, attention))\n","        else:\n","            self.body = nn.Sequential(*[SingleModule(n_channels, n_blocks, act, attention) for _ in range(n_modules)])\n","\n","        self.tail = nn.Conv2d(n_channels, n_channels, 3, 1, 1, bias=True)\n","\n","        self.upscale = nn.ModuleList([UpScale(n_channels=n_channels, scale=s, act=False) for s in scale])\n","\n","        self.output = nn.Conv2d(in_channels=n_channels, out_channels=3, kernel_size=3, stride=1, padding=1, bias=True)\n","\n","    def forward(self, x):\n","\n","        body_input = self.input(x)\n","        body_output = self.body(body_input)\n","        if self.n_modules == 1:\n","            sr_high = self.upscale[0](body_output)\n","        else:\n","            sr_high = self.upscale[0](self.tail(body_output) + body_input)\n","        results = self.output(sr_high)\n","\n","        return results\n","\n","\n","class UpScale(nn.Sequential):\n","    def __init__(self, n_channels, scale, bn=False, act=nn.ReLU(inplace=True), bias=False):\n","        layers = []\n","        if (scale & (scale - 1)) == 0:\n","            for _ in range(int(math.log(scale, 2))):\n","                layers.append(nn.Conv2d(in_channels=n_channels, out_channels=4 * n_channels, kernel_size=3, stride=1,\n","                                        padding=1, bias=bias))\n","                layers.append(nn.PixelShuffle(2))\n","                if bn: layers.append(nn.BatchNorm2d(n_channels))\n","                if act: layers.append(act)\n","        elif scale == 3:\n","            layers.append(nn.Conv2d(in_channels=n_channels, out_channels=9 * n_channels, kernel_size=3, stride=1,\n","                                    padding=1, bias=bias))\n","            layers.append(nn.PixelShuffle(3))\n","            if bn: layers.append(nn.BatchNorm2d(n_channels))\n","            if act: layers.append(act)\n","        else:\n","            raise NotImplementedError\n","\n","        super(UpScale, self).__init__(*layers)\n","\n","# discriminator class definition    \n","class Discriminator(nn.Module):\n","    \n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n","      \n","        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(64, 64, 3, stride=2, padding=1))\n","        self.conv3 = nn.utils.spectral_norm(nn.Conv2d(64, 128, 3, stride=1, padding=1))\n","        self.conv4 = nn.utils.spectral_norm(nn.Conv2d(128, 128, 3, stride=2, padding=1))\n","        self.conv5 = nn.utils.spectral_norm(nn.Conv2d(128, 256, 3, stride=1, padding=1))\n","        self.conv6 = nn.utils.spectral_norm(nn.Conv2d(256, 256, 3, stride=2, padding=1))\n","        self.conv7 = nn.utils.spectral_norm(nn.Conv2d(256, 512, 3, stride=1, padding=1))\n","        self.conv8 = nn.utils.spectral_norm(nn.Conv2d(512, 512, 3, stride=2, padding=1))\n","\n","        self.conv9 = nn.Conv2d(512, 1, 1, stride=1, padding=1)\n","        \n","        self.leakyrelu = nn.LeakyReLU(0.2, inplace=True)\n","        \n","    def forward(self, x):\n","        x = self.leakyrelu(self.conv1(x))\n","\n","        x = self.leakyrelu(self.conv2(x))\n","        x = self.leakyrelu(self.conv3(x))\n","        x = self.leakyrelu(self.conv4(x))\n","        x = self.leakyrelu(self.conv5(x))\n","        x = self.leakyrelu(self.conv6(x))\n","        x = self.leakyrelu(self.conv7(x))\n","        x = self.leakyrelu(self.conv8(x))\n","\n","        x = self.conv9(x)\n","        return F.avg_pool2d(x, x.size()[2:])#.view(x.size()[0], -1), F.avg_pool2d(x, x.size()[2:])\n","\n","# class to visualize results    \n","class Visualizer:\n","    \n","    def __init__(self, show_step=100, image_size=30):\n","        self.transform = transforms.Compose([transforms.Normalize((-1, -1, -1), (2, 2, 2)),\n","                                            transforms.ToPILImage(),\n","                                            transforms.Resize(image_size)])\n","\n","        self.show_step = show_step\n","        self.step = 0\n","\n","        self.figure, (self.lr_plot, self.hr_plot, self.fake_plot) = plt.subplots(1,3)\n","        self.figure.show()\n","\n","        self.lr_image_ph = None\n","        self.hr_image_ph = None\n","        self.fake_hr_image_ph = None\n","\n","    def show(self, inputsG, inputsD_real, inputsD_fake):\n","\n","        self.step += 1\n","        \n","        if self.step == self.show_step:\n","            i = random.randint(0, inputsG.size(0) -1)\n","\n","            lr_image = self.transform(inputsG[i])\n","            hr_image = self.transform(inputsD_real[i])\n","            fake_hr_image = self.transform(inputsD_fake[i])\n","\n","            if self.lr_image_ph is None:\n","                self.lr_image_ph = self.lr_plot.imshow(lr_image)\n","                self.hr_image_ph = self.hr_plot.imshow(hr_image)\n","                self.fake_hr_image_ph = self.fake_plot.imshow(fake_hr_image)\n","            else:\n","                self.lr_image_ph.set_data(lr_image)\n","                self.hr_image_ph.set_data(hr_image)\n","                self.fake_hr_image_ph.set_data(fake_hr_image)\n","\n","            self.figure.canvas.draw()\n","            plt.pause(0.001)\n","            \n","            self.step = 0\n","            self.figure, (self.lr_plot, self.hr_plot, self.fake_plot) = plt.subplots(1,3)\n","            self.lr_image_ph = None\n","            self.hr_image_ph = None\n","            self.fake_hr_image_ph = None\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVJDn6SBxvFW"},"source":["Mètriques\n"]},{"cell_type":"code","metadata":{"id":"aNIiEvcDxbLS","executionInfo":{"status":"ok","timestamp":1617395855308,"user_tz":-120,"elapsed":32791,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}}},"source":["# Image quality metrics\n","\n","# 1. SSIM - Structural Similarity Metric\n","from math import exp\n","  \n","# from piqa import ssim\n","\n","# class SSIM(ssim.SSIM):\n","#     def forward(self, x, y):\n","#         return 1. - super().forward(x, y)\n","\n","def gaussian(window_size, sigma):\n","    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n","    return gauss/gauss.sum()\n","\n","def create_window(window_size, channel):\n","    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n","    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n","    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n","    return window\n","\n","def _ssim(img1, img2, window, window_size, channel, size_average = True):\n","    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n","    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n","\n","    mu1_sq = mu1.pow(2)\n","    mu2_sq = mu2.pow(2)\n","    mu1_mu2 = mu1*mu2\n","\n","    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n","    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n","    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n","\n","    C1 = 0.01**2\n","    C2 = 0.03**2\n","\n","    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n","\n","    if size_average:\n","        return ssim_map.mean()\n","    else:\n","        return ssim_map.mean(1).mean(1).mean(1)\n","\n","class SSIM(torch.nn.Module):\n","    def __init__(self, window_size = 11, size_average = True):\n","        super(SSIM, self).__init__()\n","        self.window_size = window_size\n","        self.size_average = size_average\n","        self.channel = 1\n","        self.window = create_window(window_size, self.channel)\n","\n","    def forward(self, img1, img2):\n","        (_, channel, _, _) = img1.size()\n","        \n","        if channel == self.channel and self.window.data.type() == img1.data.type():\n","            window = self.window\n","        else:\n","            window = create_window(self.window_size, channel)\n","            \n","            if img1.is_cuda:\n","                window = window.cuda(img1.get_device())\n","            window = window.type_as(img1)\n","            \n","            self.window = window\n","            self.channel = channel\n","\n","\n","        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n","\n","def ssim(img1, img2, window_size = 11, size_average = True):\n","    (_, channel, _, _) = img1.size()\n","    window = create_window(window_size, channel)\n","    \n","    if img1.is_cuda:\n","        window = window.cuda(img1.get_device())\n","    window = window.type_as(img1)\n","    \n","    return _ssim(img1, img2, window, window_size, channel, size_average)\n","\n","\n","# 2. VIFP - Visual Information Fidelity Measure \n","def vifp_measure(ref, dist):\n","  \n","    ref = ref.cpu().detach().numpy()\n","    dist = dist.cpu().detach().numpy()\n","    sigma_nsq=2\n","    eps = 1e-10\n","\n","    num = 0.0\n","    den = 0.0\n","    for scale in range(1, 5):\n","       \n","        N = 2**(4-scale+1) + 1\n","        sd = N/5.0\n","\n","        if (scale > 1):\n","            ref = scipy.ndimage.gaussian_filter(ref, sd)\n","            dist = scipy.ndimage.gaussian_filter(dist, sd)\n","            ref = ref[::2, ::2]\n","            dist = dist[::2, ::2]\n","                \n","        mu1 = scipy.ndimage.gaussian_filter(ref, sd)\n","        mu2 = scipy.ndimage.gaussian_filter(dist, sd)\n","        mu1_sq = mu1 * mu1\n","        mu2_sq = mu2 * mu2\n","        mu1_mu2 = mu1 * mu2\n","        sigma1_sq = scipy.ndimage.gaussian_filter(ref * ref, sd) - mu1_sq\n","        sigma2_sq = scipy.ndimage.gaussian_filter(dist * dist, sd) - mu2_sq\n","        sigma12 = scipy.ndimage.gaussian_filter(ref * dist, sd) - mu1_mu2\n","        \n","        sigma1_sq[sigma1_sq<0] = 0\n","        sigma2_sq[sigma2_sq<0] = 0\n","        \n","        g = sigma12 / (sigma1_sq + eps)\n","        sv_sq = sigma2_sq - g * sigma12\n","        \n","        g[sigma1_sq<eps] = 0\n","        sv_sq[sigma1_sq<eps] = sigma2_sq[sigma1_sq<eps]\n","        sigma1_sq[sigma1_sq<eps] = 0\n","        \n","        g[sigma2_sq<eps] = 0\n","        sv_sq[sigma2_sq<eps] = 0\n","        \n","        sv_sq[g<0] = sigma2_sq[g<0]\n","        g[g<0] = 0\n","        sv_sq[sv_sq<=eps] = eps\n","        \n","        num += numpy.sum(numpy.log10(1 + g * g * sigma1_sq / (sv_sq + sigma_nsq)))\n","        den += numpy.sum(numpy.log10(1 + sigma1_sq / sigma_nsq))\n","        \n","    vifp = num/den\n","\n","\n","    return vifp  \n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"7bSKw2j91-bG","executionInfo":{"status":"ok","timestamp":1617395855310,"user_tz":-120,"elapsed":32789,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}}},"source":["import torchvision.transforms.functional_pil as F_pil\n","import torchvision.transforms.functional_tensor as F_t\n","\n","from torchvision.transforms.functional import crop, center_crop\n","\n","\n","def sixtenn_crop_custom(img: Tensor, size: List[int], border: List[int]) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n","\n","    if isinstance(size, numbers.Number):\n","        size = (int(size), int(size))\n","    elif isinstance(size, (tuple, list)) and len(size) == 1:\n","        size = (size[0], size[0])\n","\n","    if len(size) != 2:\n","        raise ValueError(\"Please provide only two dimensions (h, w) for size.\")\n","\n","    if isinstance(border, numbers.Number):\n","        border = (int(border), int(border))\n","    elif isinstance(border, (tuple, list)) and len(border) == 1:\n","        border = (border[0], border[0])\n","\n","    if len(border) != 2:\n","        raise ValueError(\"Please provide only two dimensions (h, w) for size.\")\n","\n","    image_width, image_height = _get_image_size(img)\n","    crop_height, crop_width = size\n","    border_height, border_width = border\n","\n","    if crop_width > image_width or crop_height > image_height:\n","        msg = \"Requested crop size {} is bigger than input size {}\"\n","        raise ValueError(msg.format(size, (image_height, image_width)))\n","\n","    tl1 = crop(img, 0, 0, crop_height + border_height, crop_width + border_width)\n","    tl2 = crop(img, 0, image_width - crop_width*3 - border_width/2, crop_height + border_height, crop_width + border_width)\n","    tl3 = crop(img, image_height - crop_height*3 - border_height/2, 0, crop_height + border_height, crop_width + border_width)\n","    tl4 = crop(img, image_height - crop_height*3 - border_height/2, image_width - crop_width*3 - border_width/2, crop_height + border_height, crop_width + border_width)\n","\n","    tr1 = crop(img, 0, image_width - crop_width*2 - border_width/2, crop_height + border_height, crop_width + border_width )\n","    tr2 = crop(img, 0, image_width - crop_width - border_width, crop_height + border_height, crop_width + border_width)\n","    tr3 = crop(img, image_height - crop_height*3 - border_height/2, image_width - crop_width*2 - border_width/2, crop_height + border_height, crop_width + border_width)\n","    tr4 = crop(img, image_height - crop_height*3 - border_height/2, image_width - crop_width - border_width, crop_height + border_height, crop_width + border_width)\n","\n","    bl1 = crop(img, image_height - crop_height*2 - border_height/2, 0, crop_height + border_height, crop_width + border_width)\n","    bl2 = crop(img, image_height - crop_height*2 - border_height/2, image_width - crop_width*3 - border_width/2, crop_height + border_height, crop_width + border_width)\n","    bl3 = crop(img, image_height - crop_height - border_height, 0, crop_height + border_height, crop_width + border_width)\n","    bl4 = crop(img, image_height - crop_height - border_height, image_width - crop_width*3 - border_width/2, crop_height + border_height, crop_width + border_width)\n","\n","    br1 = crop(img, image_height - crop_height*2 - border_height/2, image_width - crop_width*2 - border_width/2, crop_height + border_height, crop_width + border_width)\n","    br2 = crop(img, image_height - crop_height*2 - border_height/2, image_width - crop_width - border_width, crop_height + border_height, crop_width + border_width)\n","    br3 = crop(img, image_height - crop_height - border_height, image_width - crop_width*2 - border_width/2, crop_height + border_height, crop_width + border_width)\n","    br4 = crop(img, image_height - crop_height - border_height, image_width - crop_width - border_width, crop_height + border_height, crop_width + border_width)\n","\n","    # center = center_crop(img, [crop_height + border_height, crop_width + crop_width])\n","\n","    return tl1,tl2,tl3,tl4, tr1,tr2,tr3,tr4, bl1,bl2,bl3,bl4, br1,br2,br3,br4 \n","\n","class Sixteen_custom(torch.nn.Module):\n","\n","\n","    def __init__(self, size, border):\n","        super().__init__()\n","        self.size = _setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n","        self.border = _setup_size(border, error_msg=\"Please provide only two dimensions (h, w) for size.\")\n","\n","    def forward(self, img):\n","        \"\"\"\n","        Args:\n","            img (PIL Image or Tensor): Image to be cropped.\n","\n","        Returns:\n","            tuple of 5 images. Image can be PIL Image or Tensor\n","        \"\"\"\n","        return sixtenn_crop_custom(img, self.size,self.border)\n","\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(size={0})'.format(self.size)\n","\n","\n","\n","def _setup_size(size, error_msg):\n","    if isinstance(size, numbers.Number):\n","        return int(size), int(size)\n","\n","    if isinstance(size, Sequence) and len(size) == 1:\n","        return size[0], size[0]\n","\n","    if len(size) != 2:\n","        raise ValueError(error_msg)\n","\n","    return size\n","\n","def _get_image_size(img: Tensor) -> List[int]:\n","    \"\"\"Returns image sizea as (w, h)\n","    \"\"\"\n","    if isinstance(img, torch.Tensor):\n","        return F_t._get_image_size(img)\n","\n","    return F_pil._get_image_size(img)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_DjSeA2KE1S"},"source":["Prova Carrega Imàgens"]},{"cell_type":"code","metadata":{"id":"LfcHfUIdKDld","executionInfo":{"status":"ok","timestamp":1617395855311,"user_tz":-120,"elapsed":32785,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}}},"source":["from os import listdir\n","from os.path import isfile, join\n","from PIL import Image\n","from IPython.display import display\n","\n","dataset = 'TrainingSet'\n","dataroot = './drive/MyDrive/Datasets/SuperResolution/TrainingSet'\n","subfolder=\"Set/600px\"\n","\n","dataroot_real = './drive/MyDrive/Datasets/SuperResolution/TrainingSet_Real'\n","\n","# onlyfiles = [f for f in listdir(join(dataroot,subfolder)) if isfile(join(join(dataroot,subfolder), f))]\n","# pil_im = Image.open(join(join(dataroot,subfolder),onlyfiles[0]))\n","\n","# # onlyfiles = [f for f in listdir(dataroot) if isfile(join(dataroot, f))]\n","# # pil_im = Image.open(join(dataroot,onlyfiles[0]))\n","\n","\n","# display(pil_im)\n","# print(len(onlyfiles))\n","# print(pil_im.size)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9IBS_i7x-b0"},"source":["Entrenament - Carrega Dades\n"]},{"cell_type":"code","metadata":{"id":"0VAPFDGkxetq","executionInfo":{"status":"ok","timestamp":1617395855844,"user_tz":-120,"elapsed":33314,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}}},"source":["\"\"\"\n","Training the model\n","\n","\"\"\"\n","\n","\n","workers = 1\n","batchSize = 1\n","# imageSize_w = 150\n","# imageSize_h = 300\n","imageSize_w = 150\n","imageSize_h = 150\n","border= 50\n","upSampling = 4\n","factor=4\n","nEpochs = 2\n","cuda = True\n","\n","# generatorWeights = 'generator_final_ep2_13.3154.pth'\n","# discriminatorWeights = 'discriminator_final_ep2_13.3154.pth'\n","# experiment='20210317-144607_16parts_defian_mse/models'\n","\n","out = './drive/MyDrive/checkpoints'\n","\n","try:\n","    os.makedirs(out)\n","except OSError:\n","    pass\n","\n","if torch.cuda.is_available() and not cuda:\n","    print(\"WARNING: You have a CUDA device! You should probably set the variable 'cuda=True'\")\n","\n","#transform the data    \n","# transform = transforms.Compose([transforms.Resize(imageSize),\n","#                                 transforms.ToTensor()])\n","\n","normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","\n","# transform = transforms.Compose([transforms.Resize(imageSize*2),FiveCrop_custom(imageSize,border),transforms.Lambda(lambda crops: torch.stack([normalize(transforms.ToTensor()(crop)) for crop in crops[0:4]]))])\n","\n","transform = transforms.Compose([Sixteen_custom([imageSize_h,imageSize_w],border),transforms.Lambda(lambda crops: torch.stack([normalize(transforms.ToTensor()(crop)) for crop in crops]))])\n","\n","\n","# transforms.Lambda(lambda crops: torch.stack([(transforms.ToTensor()(crop)) for crop in crops]))\n","\n","# transform_real = transforms.Compose([transforms.Resize(imageSize*upSampling),\n","#                                 transforms.ToTensor()])\n","\n","# transform_real = transforms.Compose([transforms.Resize(imageSize*2*upSampling),FiveCrop_custom(imageSize*upSampling,border*upSampling),transforms.Lambda(lambda crops: torch.stack([normalize(transforms.ToTensor()(crop)) for crop in crops[0:4]]))])\n","\n","# transform_real = transforms.Compose([transforms.Resize(2400,torchvision.transforms.InterpolationMode.BICUBIC),Sixteen_custom([imageSize_h*upSampling,imageSize_w*upSampling],border*upSampling),transforms.Lambda(lambda crops: torch.stack([normalize(transforms.ToTensor()(crop)) for crop in crops]))])\n","transform_real = transforms.Compose([Sixteen_custom([imageSize_h*upSampling,imageSize_w*upSampling],border*upSampling),transforms.Lambda(lambda crops: torch.stack([normalize(transforms.ToTensor()(crop)) for crop in crops]))])\n","\n","\n","# transform = transforms.Compose([transforms.ToTensor()])\n","\n","# transform_real = transforms.Compose([transforms.ToTensor()])\n","\n","\n","\n","transform_to_save = transforms.Compose([transforms.Normalize((-1, -1, -1), (2, 2, 2)),\n","                                            transforms.ToPILImage()])\n","\n","# transform = transforms.Compose([transforms.ToTensor()])\n","\n","\n","# # load training data\n","# if dataset == 'TrainingSet':\n","#     dataset_train = datasets.ImageFolder(root=dataroot, transform=transform)\n","#     dataset_train_real = datasets.ImageFolder(root=dataroot_real, transform=transform_real)\n","#     print(\"Dataset\")\n","# assert dataset_train\n","\n","# dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batchSize,\n","#                                          shuffle=False, num_workers=int(workers))\n","\n","# dataloader_train_real = torch.utils.data.DataLoader(dataset_train_real, batch_size=batchSize,\n","#                                          shuffle=False, num_workers=int(workers))\n","\n","\n","# print(dataset_train[0][0].shape)\n","# print(dataset_train_real[0][0].shape)\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FmzRZDMyBWz"},"source":["Avaluació"]},{"cell_type":"code","metadata":{"id":"CIYOm2xVxiWa","colab":{"base_uri":"https://localhost:8080/","height":303},"executionInfo":{"status":"ok","timestamp":1617395868298,"user_tz":-120,"elapsed":45744,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}},"outputId":"0b32f61f-04c0-47ae-a6b9-abb850f392e9"},"source":["\"\"\"\n","Evaluating the model\n","\n","\"\"\"\n","\n","\n","generatorWeights = 'generator_final_ep3_13.5665.pth'\n","# discriminatorWeights = 'discriminator_final_ep1_13.5216.pth'\n","experiment_test='20210330-172437_16parts_defian_mse/models'\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","eval_generator= Generator(64, 20,10, nn.ReLU(True),attention=True, scale=[upSampling ])\n","\n","# generator.to(device)\n","\n","# Optional DataParallel, not needed for single GPU usage\n","# generator1 = torch.nn.DataParallel(generator, device_ids=[0]).to(device)\n","# Or, using default 'device_ids=None'\n","# generator1= torch.nn.DataParallel(generator).to(device)\n","\n","if generatorWeights != '':\n","    eval_generator.load_state_dict(torch.load(join(out,experiment_test,generatorWeights)))\n","    # cpk = torch.load((join(out,experiment_test,generatorWeights)), map_location={'cuda:1': 'cuda:0'})[\"state_dict\"]\n","    # eval_generator.load_state_dict(cpk, strict=False)\n","    print(\"Generator model loaded\")\n","\n","# load the trained model\n","# eval_generator = Generator(16, upSampling)\n","# eval_generator.load_state_dict(torch.load(join(out,experiment,generatorWeights)))\n","eval_generator.eval()\n","# eval_generator.half() \n","\n","dataset = 'TestSet'\n","dataroot_test = './drive/MyDrive/Datasets/SuperResolution/TestSet'\n","\n","\n","\n","workers = 1\n","batchSize = 1\n","cuda = True\n","\n","\n","if torch.cuda.is_available() and not cuda:\n","    print(\"WARNING: You have a CUDA device! You should probably set the variable 'cuda=True'\")\n","\n","# transform the data   \n","\n","# Mateixa transformació de dades que en training\n","\n","\n","\n","# load test data\n","if dataset == 'TestSet':\n","    dataset_test = datasets.ImageFolder(root=dataroot_test, transform=transform)\n","assert dataset\n","\n","dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batchSize,\n","                                         shuffle=False, num_workers=int(workers))\n","\n","\n","\n","print(dataset_test[0][0].shape)\n","# set the visualizer\n","visualizer = Visualizer(image_size=imageSize_w*upSampling)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Generator model loaded\n","torch.Size([16, 3, 200, 200])\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 3 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"GHj1Ix6zwbIm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617396547818,"user_tz":-120,"elapsed":725245,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}},"outputId":"c3b9c524-6da0-45a2-a7af-71dcfc28e2d5"},"source":["\n","print(\"Model evaluation\\n\\n\")\n","\n","\n","\n","# use GPU if available\n","if cuda:\n","    eval_generator.cuda()\n","\n","low_res = torch.FloatTensor(batchSize, 3, imageSize_w, imageSize_h)  \n","\n","  \n","for i, data in enumerate(dataloader_test):\n","    \n","    \n","    # get data\n","    # high_res_real, _ = data\n","    low_res, _ = data\n","    low_res=torch.squeeze(low_res)\n","    \n","    sample_fname, _ = dataloader_test.dataset.samples[i]\n","    print(sample_fname)\n","    head, tail = os.path.split(sample_fname)\n","    final_tail=tail[12:16]\n","    print(final_tail)\n","\n","    # downsample images to low resolution\n","    for j in range(batchSize*16):\n","        low_res_crop=low_res[j,:,:,:].unsqueeze(0)\n","        \n","      # Generate real and fake inputs\n","        if cuda:\n","            with torch.cuda.amp.autocast():\n","            # high_res_real = Variable(high_res_real.cuda())\n","               high_res_fake_crop = eval_generator(Variable(low_res_crop).cuda())\n","            \n","        \n","        else:\n","            # high_res_real = Variable(high_res_real)\n","            high_res_fake = eval_generator(Variable(low_res_crop))\n","            \n","        # ssim_val = ssim(high_res_real, high_res_fake)\n","        # mean_ssim += ssim_val\n","        \n","        # vifp = vifp_measure(high_res_real, high_res_fake)\n","        # mean_vifp += vifp\n","      \n","        try:\n","          os.makedirs(join(out,experiment_test,\"save_test\"))\n","          time.sleep(3)\n","          print(\"Sleep 3 s\")\n","        except OSError:\n","          pass\n","\n","        if j==0:\n","          image0=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image0=image0.crop((0, 0, imageSize_w*upSampling, imageSize_h*upSampling))\n","        elif j==1:\n","          image1=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image1=image1.crop((border*upSampling/2,0, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling))\n","        elif j==2:\n","          image2=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image2=image2.crop((0,border*upSampling/2, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))  \n","          \n","        elif j==3:\n","          image3=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image3=image3.crop((border*upSampling/2,border*upSampling/2, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))  \n","          \n","        elif j==4:\n","          image4=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image4=image4.crop((border*upSampling/2,0, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling))\n","        elif j==5:\n","          image5=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image5=image5.crop(( border*upSampling,0, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))\n","        elif j==6:\n","          image6=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image6=image6.crop(( border*upSampling/2,border*upSampling/2, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))\n","        elif j==7:\n","          image7=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image7=image7.crop(( border*upSampling,border*upSampling/2, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))\n","\n","        if j==8:\n","          image8=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image8=image8.crop((0, border*upSampling/2, imageSize_w*upSampling++border*upSampling, imageSize_h*upSampling+border*upSampling))\n","        elif j==9:\n","          image9=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image9=image9.crop((border*upSampling/2,border*upSampling/2, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))\n","        elif j==10:\n","          image10=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image10=image10.crop((0,border*upSampling, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))  \n","          \n","        elif j==11:\n","          image11=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image11=image11.crop((border*upSampling/2,border*upSampling, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))  \n","          \n","        elif j==12:\n","          image12=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image12=image12.crop((border*upSampling/2,border*upSampling/2, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))\n","        elif j==13:\n","          image13=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image13=image13.crop(( border*upSampling,border*upSampling/2, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))\n","        elif j==14:\n","          image14=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image14=image14.crop(( border*upSampling/2,border*upSampling, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))\n","        elif j==15:\n","          image15=transform_to_save(high_res_fake_crop.cpu().data[0]) \n","          image15=image15.crop(( border*upSampling,border*upSampling, imageSize_w*upSampling+border*upSampling, imageSize_h*upSampling+border*upSampling))\n","\n","\n","          dst = Image.new('RGB', (image0.width * 4, image0.height*4))\n","          dst.paste(image0, (0 * image0.width, 0))\n","          dst.paste(image1, (1 * image0.width, 0))\n","          dst.paste(image2, (0 * image0.width, 1 * image0.height))\n","          dst.paste(image3, (1 * image0.width, 1 * image0.height))\n","          \n","          dst.paste(image4, (2 * image0.width, 0))\n","          dst.paste(image5, (3 * image0.width, 0))\n","          dst.paste(image6, (2 * image0.width, 1 * image0.height))\n","          dst.paste(image7, (3 * image0.width, 1 * image0.height))\n","\n","          dst.paste(image8, (0 * image0.width, 2 * image0.height))\n","          dst.paste(image9, (1 * image0.width, 2 * image0.height))\n","          dst.paste(image10, (0 * image0.width, 3 * image0.height))\n","          dst.paste(image11, (1 * image0.width, 3 * image0.height))\n","          \n","          dst.paste(image12, (2 * image0.width, 2 * image0.height))\n","          dst.paste(image13, (3 * image0.width, 2 * image0.height))\n","          dst.paste(image14, (2 * image0.width, 3 * image0.height))\n","          dst.paste(image15, (3 * image0.width, 3 * image0.height))\n","\n","\n","          path_save_test=join(out,experiment_test,\"save_test\",\"candidate_%s.png\" % final_tail,)\n","          dst.save(path_save_test) \n","          print(path_save_test)\n","\n","    print('\\r[%d/%d] ' % ( i+1, len(dataloader_test)))\n","\n","    # visualizer.show(low_res, high_res_real.cpu().data, high_res_fake.cpu().data)\n","\n","print(\"Evaluation ended\")    \n","\n","    # image=transform_to_save(high_res_fake.cpu().data[0]) \n","    # image.save(join(out,stamp,\"save\",\"prova_%d.png\" % i)) \n","    \n","    \n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Model evaluation\n","\n","\n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0010.png\n","0010\n","Sleep 3 s\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0010.png\n","[1/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0030.png\n","0030\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0030.png\n","[2/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0050.png\n","0050\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0050.png\n","[3/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0070.png\n","0070\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0070.png\n","[4/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0090.png\n","0090\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0090.png\n","[5/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0110.png\n","0110\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0110.png\n","[6/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0150.png\n","0150\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0150.png\n","[7/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0170.png\n","0170\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0170.png\n","[8/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0190.png\n","0190\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0190.png\n","[9/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0210.png\n","0210\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0210.png\n","[10/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0230.png\n","0230\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0230.png\n","[11/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0250.png\n","0250\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0250.png\n","[12/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0270.png\n","0270\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0270.png\n","[13/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0290.png\n","0290\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0290.png\n","[14/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0310.png\n","0310\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0310.png\n","[15/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0330.png\n","0330\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0330.png\n","[16/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0350.png\n","0350\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0350.png\n","[17/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0370.png\n","0370\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0370.png\n","[18/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0390.png\n","0390\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0390.png\n","[19/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0410.png\n","0410\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0410.png\n","[20/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0430.png\n","0430\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0430.png\n","[21/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0450.png\n","0450\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0450.png\n","[22/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0470.png\n","0470\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0470.png\n","[23/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0510.png\n","0510\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0510.png\n","[24/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0530.png\n","0530\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0530.png\n","[25/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0550.png\n","0550\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0550.png\n","[26/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0570.png\n","0570\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0570.png\n","[27/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0590.png\n","0590\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0590.png\n","[28/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0610.png\n","0610\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0610.png\n","[29/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0630.png\n","0630\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0630.png\n","[30/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0650.png\n","0650\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0650.png\n","[31/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0670.png\n","0670\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0670.png\n","[32/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0690.png\n","0690\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0690.png\n","[33/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0711.png\n","0711\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0711.png\n","[34/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0730.png\n","0730\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0730.png\n","[35/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0750.png\n","0750\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0750.png\n","[36/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0770.png\n","0770\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0770.png\n","[37/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0790.png\n","0790\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0790.png\n","[38/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0810.png\n","0810\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0810.png\n","[39/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0830.png\n","0830\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0830.png\n","[40/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0850.png\n","0850\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0850.png\n","[41/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0870.png\n","0870\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0870.png\n","[42/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0890.png\n","0890\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0890.png\n","[43/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0910.png\n","0910\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0910.png\n","[44/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0930.png\n","0930\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0930.png\n","[45/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0950.png\n","0950\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0950.png\n","[46/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0970.png\n","0970\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0970.png\n","[47/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_0990.png\n","0990\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_0990.png\n","[48/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1010.png\n","1010\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1010.png\n","[49/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1030.png\n","1030\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1030.png\n","[50/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1050.png\n","1050\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1050.png\n","[51/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1070.png\n","1070\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1070.png\n","[52/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1090.png\n","1090\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1090.png\n","[53/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1110.png\n","1110\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1110.png\n","[54/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1130.png\n","1130\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1130.png\n","[55/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1150.png\n","1150\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1150.png\n","[56/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1170.png\n","1170\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1170.png\n","[57/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1190.png\n","1190\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1190.png\n","[58/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1210.png\n","1210\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1210.png\n","[59/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1230.png\n","1230\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1230.png\n","[60/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1250.png\n","1250\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1250.png\n","[61/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1270.png\n","1270\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1270.png\n","[62/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1290.png\n","1290\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1290.png\n","[63/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1310.png\n","1310\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1310.png\n","[64/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1330.png\n","1330\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1330.png\n","[65/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1350.png\n","1350\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1350.png\n","[66/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1370.png\n","1370\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1370.png\n","[67/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1390.png\n","1390\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1390.png\n","[68/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1410.png\n","1410\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1410.png\n","[69/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1430.png\n","1430\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1430.png\n","[70/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1450.png\n","1450\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1450.png\n","[71/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1471.png\n","1471\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1471.png\n","[72/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1490.png\n","1490\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1490.png\n","[73/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1510.png\n","1510\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1510.png\n","[74/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1530.png\n","1530\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1530.png\n","[75/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1550.png\n","1550\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1550.png\n","[76/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1570.png\n","1570\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1570.png\n","[77/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1590.png\n","1590\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1590.png\n","[78/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1610.png\n","1610\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1610.png\n","[79/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1630.png\n","1630\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1630.png\n","[80/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1650.png\n","1650\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1650.png\n","[81/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1670.png\n","1670\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1670.png\n","[82/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1690.png\n","1690\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1690.png\n","[83/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1710.png\n","1710\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1710.png\n","[84/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1730.png\n","1730\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1730.png\n","[85/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1750.png\n","1750\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1750.png\n","[86/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1770.png\n","1770\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1770.png\n","[87/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1790.png\n","1790\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1790.png\n","[88/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1810.png\n","1810\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1810.png\n","[89/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1830.png\n","1830\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1830.png\n","[90/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1850.png\n","1850\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1850.png\n","[91/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1870.png\n","1870\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1870.png\n","[92/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1890.png\n","1890\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1890.png\n","[93/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1910.png\n","1910\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1910.png\n","[94/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1930.png\n","1930\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1930.png\n","[95/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1950.png\n","1950\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1950.png\n","[96/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1970.png\n","1970\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1970.png\n","[97/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_1990.png\n","1990\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_1990.png\n","[98/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_2069.png\n","2069\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_2069.png\n","[99/100] \n","./drive/MyDrive/Datasets/SuperResolution/TestSet/Set/image_600px_2105.png\n","2105\n","./drive/MyDrive/checkpoints/20210330-172437_16parts_defian_mse/models/save_test/candidate_2105.png\n","[100/100] \n","Evaluation ended\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0BXz1D4QdB-0","executionInfo":{"status":"ok","timestamp":1617396547823,"user_tz":-120,"elapsed":725246,"user":{"displayName":"Marc Martínez","photoUrl":"","userId":"08693480389513132956"}}},"source":[""],"execution_count":10,"outputs":[]}]}